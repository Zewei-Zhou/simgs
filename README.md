<h3 align="center"><strong>ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting</strong></h3>

<p align="center">
  <a href="https://ruijiezhu94.github.io/ruijiezhu/">Ruijie Zhu</a><sup>1,2</sup>,</span>
  <a href="https://mulinyu.github.io/">Mulin Yu</a><sup>2</sup>,</span>
  <a href="https://eveneveno.github.io/lnxu">Linning Xu</a><sup>3</sup>,</span>
  <a href="https://jianglh-whu.github.io/">Lihan Jiang</a><sup>1,2</sup>,</span>
  <a href="https://yixuanli98.github.io/">Yixuan Li</a><sup>3</sup>,</span><br> 
  <a href="https://staff.ustc.edu.cn/~tzzhang/">Tianzhu Zhang</a><sup>1</sup>,</span>
  <a href="https://oceanpang.github.io/">Jiangmiao Pang</a><sup>2</sup>,</span>
  <a href="https://daibo.info/">Bo Dai</a><sup>4</sup></span>
  <br>
  <sup>1</sup> USTC </span> 
  <sup>2</sup> Shanghai AI Lab </span> 
  <sup>3</sup> CUHK </span> 
  <sup>4</sup> HKU </span>
  <br>
  <b>ICCV 2025</b>
</p>

<div align="center">
  <a href='https://arxiv.org/abs/[]'><img src='https://img.shields.io/badge/arXiv-2507.15454-b31b1b.svg'></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <a href='https://ruijiezhu94.github.io/ObjectGS_page/'><img src='https://img.shields.io/badge/Project-Page-orange'></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <!-- <a href='https://ruijiezhu94.github.io/ObjectGS_page/'><img src='https://img.shields.io/badge/YouTube-Demo-yellow'></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  -->
  <a href='https://github.com/RuijieZhu94/ObjectGS?tab=MIT-1-ov-file'><img src='https://img.shields.io/badge/License-MIT-green'></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <a href='https://ruijiezhu94.github.io/ObjectGS'><img src="https://visitor-badge.laobi.icu/badge?page_id=ruijiezhu94.objectgs"/></a>
  <br>
  <br>
</div>

<br>

<p align="center">
<img src="assets/pipeline.jpg" width="97%"/>
</p>

> The overall architecture of ObjectGS. We first use a 2D segmentation pipeline to assign object ID and lift it to 3D. Then we initialize the anchors and use them to generate object-aware neural Gaussians. To provide semantic guidance, we model the Gaussian semantics and construct classification-based constraints. As a result, our method enables both object-level and scene-level reconstruction.


## ğŸš€ Quick Start

### ğŸ”§ Dataset Preparation
To train ObjectGS, you should download the following dataset:
* [3DOVS](https://github.com/Kunhao-Liu/3D-OVS)
* [LERF-Mask](https://github.com/lkeab/gaussian-grouping/blob/main/docs/dataset.md#1-lerf-mask-dataset)
* [Replica](https://github.com/facebookresearch/Replica-Dataset)
* [Scannet++](https://github.com/scannetpp/scannetpp)

Or you can use our processed subsets:
[Google Drive](https://drive.google.com/drive/folders/1rKeDAactXtjqb37PiJ68ia-KyoKehUmy?usp=sharing)

We organize the datasets as follows:

```shell
â”œâ”€â”€ data
â”‚   | 3dovs
â”‚     â”œâ”€â”€ bed
â”‚     â”œâ”€â”€ bench
â”‚     â”œâ”€â”€ ...
â”‚   | lerf_mask
â”‚     â”œâ”€â”€ figurines
â”‚     â”œâ”€â”€ ramen
â”‚     â”œâ”€â”€ teatime
â”‚   | replica
â”‚     â”œâ”€â”€ office_0
â”‚     â”œâ”€â”€ office_1
â”‚     â”œâ”€â”€ ...
â”‚   | scannetpp_ovs
â”‚     â”œâ”€â”€ 09bced689e
â”‚     â”œâ”€â”€ 0d2ee665be
â”‚     â”œâ”€â”€ ...
```


### ğŸ› ï¸ Installation
1. Clone this repo:
```bash
git clone git@github.com:RuijieZhu94/ObjectGS.git --recursive
```
2. Install dependencies:
```bash
cd ObjectGS
conda env create --file environment.yml
```

### ğŸŒŸ Training

#### Data Preprocessing
##### Step1: Generate Dynamic Masks
Convert your video into image sequences and save them in a folder images.
```shell
seq_path/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ 00001.jpg
â”‚   â”œâ”€â”€ 00002.jpg
â”‚   â”œâ”€â”€ ...
```
Then, run the following command to generate dynamic masks.
```shell
cd preprocess/tools/
bash generate_mask.sh $seq_path
```

##### Step2: Run SfM to reconstruct the camera poses and point cloud prior
Install neccessary packages.
```shell
conda install -c conda-forge cudatoolkit colmap glomap
```
Then run the following command to run SfM.
```shell
bash run_sfm.sh $seq_path
```
The SfM results will be saved in the folder sparse under the sequence path.
##### Step3: Generate depth prior
```shell
bash generate_depth.sh $seq_path
```
We use monocular depth estimation to generate depth prior for the scene. These extra geometry clues will help us reconstruct the scene more accurately. The reconstructed mesh will further serve as the foundation for agent-scene interaction and collision detection in the following RL training stage. The depth results will be saved in the depth folder depths under the sequence path.

##### Step4: Point Cloud Preprocessing
Preprocessing 3D point clouds with semantic labels by projecting them onto 2D images and assigning colors/labels through various voting strategies.
```shell
python ply_preprocessing.py
```

#### Model Training

```shell
# 3dgs version
bash train_3d.sh /path/to/your/dataset
# 2dgs version
bash train_2d.sh /path/to/your/dataset
```


### ğŸ‡ Evaluation 

#### Rendering 2D Semantics

```shell
bash render.sh path/to/your/training/folder
```

#### Rendering 3D Semantics (Point Cloud)

```shell
python vis_ply.py
```

#### Open-Vocabulary Segmentation Evaluation

Please refer to [Gaussian-Grouping](https://github.com/lkeab/gaussian-grouping/blob/main/docs/dataset.md#2-render-mask-with-text-prompt) for OVS evaluation.

#### Exporting Mesh
```shell
# exporting scene mesh
python export_mesh.py -m path/to/your/training/folder

# exporting object mesh
python export_object_mesh.py -m path/to/your/training/folder --query_label_id -1
## query_label_id: specify a object id (0~255), -1 for all objects
```

### ğŸ“œ Citation

If you find our work useful, please cite:

```bibtex
@inproceedings{zhu2025objectgs,
  title={ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting},
  author={Zhu, Ruijie and Yu, Mulin and Xu, Linning and Jiang, Lihan and Li, Yixuan and Zhang, Tianzhu and Pang, Jiangmiao and Dai, Bo},
  booktitle={International Conference on Computer Vision (ICCV), 2025},
  year={2025}
}
```

### ğŸ¤ Acknowledgements
Our code is based on [Scaffold-GS](https://city-super.github.io/scaffold-gs/), [HorizonGS](https://city-super.github.io/horizon-gs/), [GSplat](https://github.com/nerfstudio-project/gsplat) and [Gaussian-Grouping](https://github.com/lkeab/gaussian-grouping). We thank the authors for their excellent work!

